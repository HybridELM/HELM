{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82ddeecf-6406-4139-bd84-e00290a228fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"list_data.json\") as json_file:\n",
    "    json_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3293b7ce-8bbd-4cb7-8398-54b8e44137f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##DEFINE FUNCTIONS\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense,Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# set gpu\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "start_time = time.time()\n",
    "error_log = []\n",
    "LSTM_Forecasts = []\n",
    "\n",
    "def smape_loss(y_true, y_pred):\n",
    "    horizon = y_pred.shape[1]\n",
    "    numer = tf.abs(y_pred - y_true)\n",
    "    denom = tf.abs(y_pred) + tf.abs(y_true) + tf.keras.backend.epsilon()\n",
    "    return (200 / horizon) * tf.reduce_sum(numer / denom)\n",
    "\n",
    "def run_LSTM(ts, horizon, n_steps, cells, learning_rate, epochs, patience, min_delta, dropout_rate):\n",
    "    # Differencing the time series data\n",
    "    #diff_ts = difference(ts)\n",
    "   # diff_ts = np.array(diff_ts)\n",
    "    n = len(ts)\n",
    "    n_steps = n_steps\n",
    "    horizon = horizon\n",
    "    array_size = n - n_steps - horizon\n",
    "\n",
    "    x_train = np.zeros((array_size, n_steps, 1))\n",
    "    y_train = np.zeros((array_size, horizon))\n",
    "    x_test = np.zeros((1, n_steps, 1))\n",
    "    y_test = np.zeros((1, horizon))\n",
    "\n",
    "    for i in range(array_size):\n",
    "        x_train[i, :, :] = ts[i:(i+n_steps)].reshape(-1, 1)\n",
    "        y_train[i, :] = ts[(i+n_steps):(i+n_steps+horizon)]\n",
    "\n",
    "    y_test[0, :] = ts[(n-horizon):n]\n",
    "    x_test[0, :, :] = ts[(array_size):(n-horizon)].reshape(-1, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(cells, input_shape=(n_steps, 1), unroll=False))\n",
    "    # model.add(Dropout(dropout_rate))\n",
    "    # model.add(LSTM(cells, return_sequences=True))\n",
    "    # model.add(Dropout(dropout_rate))\n",
    "    # model.add(LSTM(cells, return_sequences=True))\n",
    "    # model.add(Dropout(dropout_rate))\n",
    "    # model.add(LSTM(cells))\n",
    "    model.add(Dense(horizon))\n",
    "\n",
    "    model.compile(loss=smape_loss, optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=epochs, verbose=0, callbacks=[EarlyStopping(monitor='loss', patience=patience, min_delta=min_delta)])\n",
    "\n",
    "    preds= model.predict(x_test)\n",
    "\n",
    "    # De-differencing the forecasts\n",
    "    #preds = inverse_difference(ts[-n_steps:], preds_diff[0])\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e918d1b-4fab-469e-868b-8a282d8cfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "##PARAMS\n",
    "\n",
    "horizon = 12\n",
    "n_steps = 25\n",
    "cells = 512\n",
    "#cells_2=512 #if you want this want, you have to add return_sequences=True to the first lstm layer and uncomment the second lstm layer\n",
    "#cells_3=512\n",
    "#cells_4=512\n",
    "learning_rate = 0.35\n",
    "epochs = 1000\n",
    "patience = 10\n",
    "min_delta = 0.01\n",
    "dropout_rate=0.071\n",
    "batch_size=32\n",
    "\n",
    "#\n",
    "which_series=range(0,1428) #change which series, via range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4282c42b-c497-4d5d-a152-20c9d3a45ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "###runs the lstm\n",
    "\n",
    "all_forecasts_1 = []\n",
    "\n",
    "for i in which_series:\n",
    "    ts = np.array(json_data[i]['target'])\n",
    "    series_forecasts = []\n",
    "    print(\"i am at \"+ str(i))\n",
    "    for _ in range(3):\n",
    "        with tf.device('/GPU:0'):\n",
    "            forecasts = run_LSTM(ts, horizon, n_steps, cells, learning_rate, epochs, patience, min_delta, dropout_rate)\n",
    "            series_forecasts.append(forecasts)\n",
    "            \n",
    "    series_forecasts = np.array(series_forecasts)\n",
    "    median_forecasts = np.median(series_forecasts, axis=0)\n",
    "    all_forecasts_1.append(median_forecasts)\n",
    "\n",
    "all_forecasts_1 = np.array(all_forecasts_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8d1277d2-b809-4426-bff1-92b2a82c0541",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "\n",
    "def smape_loss(y_true, y_pred):\n",
    "    horizon = y_pred.shape[1]\n",
    "    numer = tf.abs(y_pred - y_true)\n",
    "    denom = tf.abs(y_pred) + tf.abs(y_true) + tf.keras.backend.epsilon()\n",
    "    return (200 / horizon) * tf.reduce_sum(numer / denom)\n",
    "\n",
    "def exponential_decay(initial_lr, decay_rate):\n",
    "    def scheduler(epoch, lr):\n",
    "        return lr * decay_rate\n",
    "    return LearningRateScheduler(scheduler)\n",
    "\n",
    "def standardize_data(data):\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    standardized = (data - min_val) / (max_val - min_val)\n",
    "    return standardized, min_val, max_val\n",
    "\n",
    "def denormalize_data(data, min_val, max_val):\n",
    "    denormalized = data * (max_val - min_val) + min_val\n",
    "    return denormalized\n",
    "\n",
    "def run_LSTM(ts, horizon, n_steps, cells, initial_lr, epochs, patience, min_delta, dropout_rate, batch_size):\n",
    "    temp_ts=ts\n",
    "    ts, mean, std = standardize_data(ts)  # Standardize the time series data\n",
    "\n",
    "    n = len(ts)\n",
    "    n_steps = n_steps\n",
    "    horizon = horizon\n",
    "    array_size = n - n_steps - horizon\n",
    "\n",
    "    x_train = np.zeros((array_size, n_steps, 1))\n",
    "    y_train = np.zeros((array_size, horizon))\n",
    "    x_test = np.zeros((1, n_steps, 1))\n",
    "    y_test = np.zeros((1, horizon))\n",
    "\n",
    "    for i in range(array_size):\n",
    "        x_train[i, :, :] = ts[i:(i+n_steps)].reshape(-1, 1)\n",
    "        y_train[i, :] = ts[(i+n_steps):(i+n_steps+horizon)]\n",
    "\n",
    "    y_test[0, :] = ts[(n-horizon):n]\n",
    "    x_test[0, :, :] = ts[(array_size):(n-horizon)].reshape(-1, 1)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(cells, input_shape=(n_steps, 1), unroll=False))\n",
    "    #model.add(Dropout(dropout_rate))\n",
    "    #model.add(LSTM(cells))\n",
    "    model.add(Dense(horizon))\n",
    "\n",
    "    model.compile(loss=smape_loss, optimizer=Adam(learning_rate=initial_lr))\n",
    "\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='loss', patience=patience, min_delta=min_delta)]\n",
    "\n",
    "    model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=callbacks)\n",
    "\n",
    "    preds_diff = model.predict(x_test)\n",
    "\n",
    "    preds = denormalize_data(preds_diff, mean, std)  # Transform the forecasts back to the original scale\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e848ea-bac3-4dfb-b541-012a7fb47e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "##This prints sMAPES for 1 horizon\n",
    "\n",
    "smape_horizon=12 #Change this for w/e horizon you are running in the lstm\n",
    "smapes_1=[]\n",
    "for i in range(0,all_forecasts_1.shape[0]):\n",
    "    temp = np.array(json_data[which_series[i]]['target'])\n",
    "    y_pred=all_forecasts_1[0].flatten()\n",
    "    y_true=temp[-smape_horizon:]\n",
    "    horizon = y_true.shape[0]\n",
    "    y_pred = tf.cast(y_pred, dtype=tf.float32)\n",
    "    y_true = tf.cast(y_true, dtype=tf.float32)\n",
    "    numer =  tf.abs(y_pred - y_true)\n",
    "    denom = tf.abs(y_pred) + tf.abs(y_true) + tf.keras.backend.epsilon()\n",
    "    smapes_1.append((200 / horizon) * (numer/denom).numpy().sum())\n",
    "smapes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e008ca-6742-4d0c-980c-aec79a22acdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Don't forget to change horizon name\n",
    "import pickle\n",
    "with open(\"Capstone\\\\R\\\\LSTM_Forecasts\\\\LSTM_Forecast_12.pickle\", 'wb') as f:\n",
    "    pickle.dump(all_forecasts_1, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63b8935-71d9-405d-bb4f-b4a9953500cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "folder_path = \"Capstone\\\\R\\\\LSTM_Forecasts\"\n",
    "pickle_files = [f for f in os.listdir(folder_path) if f.endswith('.pickle')]\n",
    "pickle_files.sort(key=lambda x: int(''.join(filter(str.isdigit, x))))\n",
    "\n",
    "combined_data = []\n",
    "for file_name in pickle_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    with open(file_path, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "        combined_data.extend(data)\n",
    "\n",
    "combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9086294c-7a66-41d9-b713-23b39c80ab98",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data = []\n",
    "file_path = os.path.join(folder_path, temp_pickle)\n",
    "with open(file_path, 'rb') as file:\n",
    "    data = pickle.load(file)\n",
    "    combined_data.extend(data)\n",
    "combined_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
